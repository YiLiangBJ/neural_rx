#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿèƒ½åŠ›éªŒè¯è„šæœ¬
æ£€æŸ¥ CPUã€å†…å­˜ã€GPUã€CUDA å’Œ cuDNN ç­‰ç¡¬ä»¶å’Œè½¯ä»¶ç¯å¢ƒ
"""

import sys
import os
import platform
import psutil

# Windows ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    if sys.stdout.encoding != 'utf-8':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


def get_size(bytes, suffix="B"):
    """å°†å­—èŠ‚è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ ¼å¼"""
    factor = 1024
    for unit in ["", "K", "M", "G", "T", "P"]:
        if bytes < factor:
            return f"{bytes:.2f}{unit}{suffix}"
        bytes /= factor


def verify_system_info():
    """éªŒè¯ç³»ç»ŸåŸºæœ¬ä¿¡æ¯"""
    print("=" * 60)
    print("ç³»ç»Ÿä¿¡æ¯")
    print("=" * 60)
    
    uname = platform.uname()
    print(f"ç³»ç»Ÿ: {uname.system}")
    print(f"èŠ‚ç‚¹åç§°: {uname.node}")
    print(f"å‘è¡Œç‰ˆæœ¬: {uname.release}")
    print(f"ç‰ˆæœ¬: {uname.version}")
    print(f"æœºå™¨ç±»å‹: {uname.machine}")
    print(f"å¤„ç†å™¨: {uname.processor if uname.processor else platform.processor()}")
    
    # Python ä¿¡æ¯
    print(f"\nPython ç‰ˆæœ¬: {sys.version}")
    print(f"Python è·¯å¾„: {sys.executable}")


def verify_cpu_info():
    """éªŒè¯ CPU ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("CPU ä¿¡æ¯")
    print("=" * 60)
    
    # CPU æ ¸å¿ƒæ•°
    print(f"ç‰©ç†æ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)}")
    print(f"é€»è¾‘æ ¸å¿ƒæ•° (å«è¶…çº¿ç¨‹): {psutil.cpu_count(logical=True)}")
    
    # CPU é¢‘ç‡
    try:
        cpufreq = psutil.cpu_freq()
        if cpufreq:
            print(f"æœ€å¤§é¢‘ç‡: {cpufreq.max:.2f} MHz")
            print(f"æœ€å°é¢‘ç‡: {cpufreq.min:.2f} MHz")
            print(f"å½“å‰é¢‘ç‡: {cpufreq.current:.2f} MHz")
    except Exception:
        print("æ— æ³•è·å– CPU é¢‘ç‡ä¿¡æ¯")
    
    # CPU ä½¿ç”¨ç‡
    print(f"\nCPU æ€»ä½“ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1)}%")
    
    # æ¯ä¸ªæ ¸å¿ƒçš„ä½¿ç”¨ç‡
    print("å„æ ¸å¿ƒä½¿ç”¨ç‡:")
    for i, percentage in enumerate(psutil.cpu_percent(percpu=True, interval=1)):
        print(f"  æ ¸å¿ƒ {i}: {percentage}%")


def verify_memory_info():
    """éªŒè¯å†…å­˜ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("å†…å­˜ä¿¡æ¯")
    print("=" * 60)
    
    # RAM ä¿¡æ¯
    svmem = psutil.virtual_memory()
    print(f"æ€»å†…å­˜: {get_size(svmem.total)}")
    print(f"å¯ç”¨å†…å­˜: {get_size(svmem.available)}")
    print(f"å·²ç”¨å†…å­˜: {get_size(svmem.used)} ({svmem.percent}%)")
    
    # SWAP ä¿¡æ¯
    swap = psutil.swap_memory()
    print(f"\nSWAP æ€»é‡: {get_size(swap.total)}")
    print(f"SWAP å¯ç”¨: {get_size(swap.free)}")
    print(f"SWAP å·²ç”¨: {get_size(swap.used)} ({swap.percent}%)")


def verify_disk_info():
    """éªŒè¯ç£ç›˜ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ç£ç›˜ä¿¡æ¯")
    print("=" * 60)
    
    partitions = psutil.disk_partitions()
    for partition in partitions:
        print(f"\nè®¾å¤‡: {partition.device}")
        print(f"  æŒ‚è½½ç‚¹: {partition.mountpoint}")
        print(f"  æ–‡ä»¶ç³»ç»Ÿç±»å‹: {partition.fstype}")
        try:
            partition_usage = psutil.disk_usage(partition.mountpoint)
            print(f"  æ€»ç©ºé—´: {get_size(partition_usage.total)}")
            print(f"  å·²ç”¨: {get_size(partition_usage.used)}")
            print(f"  å¯ç”¨: {get_size(partition_usage.free)}")
            print(f"  ä½¿ç”¨ç‡: {partition_usage.percent}%")
        except PermissionError:
            print("  æ— æƒé™è®¿é—®æ­¤åˆ†åŒº")


def verify_tensorflow_gpu():
    """éªŒè¯ TensorFlow GPU é…ç½®"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥ TensorFlow GPU æ”¯æŒ")
    print("=" * 60)
    
    try:
        import tensorflow as tf
        print(f"âœ… TensorFlow ç‰ˆæœ¬: {tf.__version__}")
        
        # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
        gpus = tf.config.list_physical_devices('GPU')
        print(f"\n{'âœ…' if len(gpus) > 0 else 'âŒ'} GPU å¯ç”¨: {len(gpus) > 0}")
        
        if gpus:
            print(f"   æ£€æµ‹åˆ° {len(gpus)} å— GPU:")
            for i, gpu in enumerate(gpus):
                print(f"   - GPU {i}: {gpu.name}")
            
            # æ‰“å° CUDA å’Œ cuDNN ç‰ˆæœ¬
            build_info = tf.sysconfig.get_build_info()
            cuda_version = build_info.get('cuda_version', 'N/A')
            cudnn_version = build_info.get('cudnn_version', 'N/A')
            
            print(f"\n   CUDA ç‰ˆæœ¬: {cuda_version}")
            print(f"   cuDNN ç‰ˆæœ¬: {cudnn_version}")
            
            # æµ‹è¯•ç®€å•æ“ä½œ
            try:
                with tf.device('/GPU:0'):
                    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
                    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
                    c = tf.matmul(a, b)
                print(f"\n   âœ… GPU è®¡ç®—æµ‹è¯•æˆåŠŸ")
            except Exception as e:
                print(f"\n   âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        else:
            print("   æœªæ£€æµ‹åˆ° GPU,ä½¿ç”¨ CPU æ¨¡å¼")
            
    except ImportError:
        print("âŒ TensorFlow æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ TensorFlow æ—¶å‡ºé”™: {e}")
        return False
    
    return len(gpus) > 0 if gpus else False


def verify_pytorch_gpu():
    """éªŒè¯ PyTorch GPU é…ç½®"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥ PyTorch GPU æ”¯æŒ")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
        cuda_available = torch.cuda.is_available()
        print(f"\n{'âœ…' if cuda_available else 'âŒ'} GPU å¯ç”¨: {cuda_available}")
        
        if cuda_available:
            device_count = torch.cuda.device_count()
            print(f"   æ£€æµ‹åˆ° {device_count} å— GPU:")
            
            for i in range(device_count):
                print(f"   - GPU {i}: {torch.cuda.get_device_name(i)}")
                
            # æ‰“å° CUDA ç‰ˆæœ¬
            cuda_version = torch.version.cuda
            print(f"\n   CUDA ç‰ˆæœ¬: {cuda_version}")
            
            # æ‰“å° cuDNN ç‰ˆæœ¬
            if torch.backends.cudnn.enabled:
                cudnn_version = torch.backends.cudnn.version()
                print(f"   cuDNN ç‰ˆæœ¬: {cudnn_version}")
                print(f"   cuDNN å·²å¯ç”¨: {torch.backends.cudnn.enabled}")
            else:
                print(f"   cuDNN æœªå¯ç”¨")
            
            # æµ‹è¯•ç®€å•æ“ä½œ
            try:
                x = torch.rand(3, 3).cuda()
                y = torch.rand(3, 3).cuda()
                z = torch.matmul(x, y)
                print(f"\n   âœ… GPU è®¡ç®—æµ‹è¯•æˆåŠŸ")
            except Exception as e:
                print(f"\n   âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        else:
            print("   æœªæ£€æµ‹åˆ° GPU,ä½¿ç”¨ CPU æ¨¡å¼")
            
    except ImportError:
        print("âš ï¸  PyTorch æœªå®‰è£… (æœ¬é¡¹ç›®ä¸éœ€è¦)")
        return None
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ PyTorch æ—¶å‡ºé”™: {e}")
        return False
    
    return cuda_available


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ğŸ” ç³»ç»Ÿèƒ½åŠ›éªŒè¯å·¥å…·")
    print("=" * 60 + "\n")
    
    # ç³»ç»Ÿä¿¡æ¯
    verify_system_info()
    
    # CPU ä¿¡æ¯
    verify_cpu_info()
    
    # å†…å­˜ä¿¡æ¯
    verify_memory_info()
    
    # ç£ç›˜ä¿¡æ¯
    verify_disk_info()
    
    # TensorFlow GPU
    tf_has_gpu = verify_tensorflow_gpu()
    
    # PyTorch GPU
    pt_has_gpu = verify_pytorch_gpu()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("éªŒè¯æ€»ç»“")
    print("=" * 60)
    
    # ç³»ç»Ÿæ‘˜è¦
    uname = platform.uname()
    print(f"âœ… ç³»ç»Ÿ: {uname.system} {uname.release}")
    print(f"âœ… CPU: {psutil.cpu_count(logical=False)} æ ¸å¿ƒ / {psutil.cpu_count(logical=True)} çº¿ç¨‹")
    
    svmem = psutil.virtual_memory()
    print(f"âœ… å†…å­˜: {get_size(svmem.total)} ({get_size(svmem.available)} å¯ç”¨)")
    
    # AI æ¡†æ¶æ”¯æŒ
    if tf_has_gpu:
        print("âœ… TensorFlow GPU æ”¯æŒæ­£å¸¸")
    else:
        print("âš ï¸  TensorFlow æœªæ£€æµ‹åˆ° GPU (å°†ä½¿ç”¨ CPU)")
    
    if pt_has_gpu is not None:
        if pt_has_gpu:
            print("âœ… PyTorch GPU æ”¯æŒæ­£å¸¸")
        else:
            print("âš ï¸  PyTorch æœªæ£€æµ‹åˆ° GPU (å°†ä½¿ç”¨ CPU)")
    else:
        print("â„¹ï¸  PyTorch æœªå®‰è£…")
    
    # æ€§èƒ½è¯„ä¼°
    print("\n" + "=" * 60)
    print("æ€§èƒ½è¯„ä¼°")
    print("=" * 60)
    
    cpu_cores = psutil.cpu_count(logical=False)
    total_mem_gb = svmem.total / (1024**3)
    
    if tf_has_gpu or (pt_has_gpu if pt_has_gpu is not None else False):
        print("ğŸš€ æ¨èç”¨é€”: æ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç† (GPU åŠ é€Ÿ)")
    elif cpu_cores >= 8 and total_mem_gb >= 16:
        print("ğŸ’» æ¨èç”¨é€”: æ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç† (CPU æ¨¡å¼)")
    elif cpu_cores >= 4 and total_mem_gb >= 8:
        print("ğŸ“Š æ¨èç”¨é€”: å°è§„æ¨¡è®­ç»ƒã€æ¨ç†å’Œå®éªŒ")
    else:
        print("âš ï¸  ç¡¬ä»¶é…ç½®è¾ƒä½,å»ºè®®ä»…ç”¨äºä»£ç å¼€å‘å’Œè°ƒè¯•")
    
    print("\n" + "=" * 60)
    
    # è¿”å›çŠ¶æ€ç 
    return 0


if __name__ == "__main__":
    sys.exit(main())
